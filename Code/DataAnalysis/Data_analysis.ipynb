{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMJtV2DmFLVx"
      },
      "source": [
        "# Run always"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjyRQulwEfmx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install keras_nlp\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gf4zC6POkRH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Thesis/CODE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja3Q0QkKOxmh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "import csv\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import time\n",
        "import multiprocessing\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from datetime import datetime\n",
        "\n",
        "from hyperparameters import hyperparams_features, hyperparams\n",
        "from data_loader import load_erisk_data\n",
        "from load_save_model import load_saved_model_weights\n",
        "from feature_encoders import encode_emotions, encode_pronouns, encode_stopwords, encode_liwc_categories\n",
        "from resource_loader import load_NRC, load_LIWC, load_vocabulary, load_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNMUHpdOPRxs"
      },
      "outputs": [],
      "source": [
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boyffDHKmw9c"
      },
      "source": [
        "Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWSvsxUtPeUy"
      },
      "outputs": [],
      "source": [
        "task = \"Self-Harm\"\n",
        "\n",
        "writings_df_sh = pd.read_pickle(root_dir + \"/Processed Data/tokenized_df_\" + task + \".pkl\")\n",
        "\n",
        "#CREATE VOCABULARY, PROCESS DATA, DATAGENERATOR\n",
        "user_level_data_sh, subjects_split_sh, vocabulary_sh = load_erisk_data(writings_df_sh,train_prop= 1,\n",
        "                                                           hyperparams_features=hyperparams_features,\n",
        "                                                           logger=None, by_subset=True)\n",
        "\n",
        "print(f\"There are {len(user_level_data_sh)} subjects, of which {len(subjects_split_sh['train'])} train and {len(subjects_split_sh['test'])} test.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld1tOJuWT0-n"
      },
      "outputs": [],
      "source": [
        "task = \"Anorexia\"\n",
        "\n",
        "writings_df_a = pd.read_pickle(root_dir + \"/Processed Data/tokenized_df_\" + task + \".pkl\")\n",
        "\n",
        "#CREATE VOCABULARY, PROCESS DATA, DATAGENERATOR\n",
        "user_level_data_a, subjects_split_a, vocabulary_a = load_erisk_data(writings_df_a,train_prop= 1,\n",
        "                                                           hyperparams_features=hyperparams_features,\n",
        "                                                           logger=None, by_subset=True)\n",
        "\n",
        "print(f\"There are {len(user_level_data_a)} subjects, of which {len(subjects_split_a['train'])} train and {len(subjects_split_a['test'])} test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO9ESFouT16e"
      },
      "outputs": [],
      "source": [
        "task = \"Depression\"\n",
        "\n",
        "writings_df_d = pd.read_pickle(root_dir + \"/Processed Data/tokenized_df_\" + task + \".pkl\")\n",
        "\n",
        "#CREATE VOCABULARY, PROCESS DATA, DATAGENERATOR\n",
        "user_level_data_d, subjects_split_d, vocabulary_d = load_erisk_data(writings_df_d,train_prop= 1,\n",
        "                                                           hyperparams_features=hyperparams_features,\n",
        "                                                           logger=None, by_subset=True)\n",
        "\n",
        "print(f\"There are {len(user_level_data_d)} subjects, of which {len(subjects_split_d['train'])} train and {len(subjects_split_d['test'])} test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2EHFKxCW-L"
      },
      "source": [
        "# Basic data analysis for Data Chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlqR47ZLWxNz"
      },
      "source": [
        "Choosing Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60m28C_2VpEJ"
      },
      "outputs": [],
      "source": [
        "task = \"Depression\"\n",
        "\n",
        "if task == \"Self-Harm\":\n",
        "  writings_df = writings_df_sh\n",
        "  user_level_data = user_level_data_sh\n",
        "  subjects_split = subjects_split_sh\n",
        "elif task == \"Anorexia\":\n",
        "  writings_df = writings_df_a\n",
        "  user_level_data = user_level_data_a\n",
        "  subjects_split = subjects_split_a\n",
        "elif task == \"Depression\":\n",
        "  writings_df = writings_df_d\n",
        "  user_level_data = user_level_data_d\n",
        "  subjects_split = subjects_split_d\n",
        "else:\n",
        "  raise Exception(\"Unkown data set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNMAFGIPOfSD"
      },
      "source": [
        "\n",
        "\n",
        "Analyzing text lengths & number of texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ4VIVMyOex0"
      },
      "outputs": [],
      "source": [
        "n_texts_train = []\n",
        "text_lengths_train = []\n",
        "\n",
        "n_texts_test = []\n",
        "text_lengths_test = []\n",
        "n_train =0\n",
        "n_test = 0\n",
        "\n",
        "text_lengths_train_pos = []\n",
        "text_lengths_train_neg = []\n",
        "train_pos = 0\n",
        "train_neg = 0\n",
        "\n",
        "for user, user_data in user_level_data.items():\n",
        "  #test\n",
        "  if len(user) >= 4 and user[-4:] == '0000':\n",
        "    n_test+=1\n",
        "    n_texts_test.append(len(user_data['texts']))\n",
        "\n",
        "    for text in user_data['texts']:\n",
        "        text_lengths_test.append(len(text))\n",
        "\n",
        "  #train\n",
        "  else:\n",
        "    n_train+=1\n",
        "    n_texts_train.append(len(user_data['texts']))\n",
        "\n",
        "    if user_data['label'] ==1:\n",
        "      train_pos+=1\n",
        "      for text in user_data['texts']:\n",
        "        text_lengths_train_pos.append(len(text))\n",
        "    else:\n",
        "      train_neg+=1\n",
        "      for text in user_data['texts']:\n",
        "        text_lengths_train_neg.append(len(text))\n",
        "\n",
        "    for text in user_data['texts']:\n",
        "      text_lengths_train.append(len(text))\n",
        "\n",
        "print(f\"Data set: {task}\")\n",
        "print(f\"Average number of texts: {np.mean(np.concatenate([n_texts_test, n_texts_train]))}\")\n",
        "print(f\"Average number of texts test: {np.mean(n_texts_test)}\")\n",
        "print(f\"Average number of texts train: {np.mean(n_texts_train)}\")\n",
        "\n",
        "print(f\"Average text length: {np.mean(np.concatenate([text_lengths_test, text_lengths_train]))}\")\n",
        "print(f\"Average text length test: {np.mean(text_lengths_test)}\")\n",
        "print(f\"Average text length train: {np.mean(text_lengths_train)}\")\n",
        "\n",
        "print(f\"Average text length train POSITIVE: {np.mean(text_lengths_train_pos)}\")\n",
        "print(f\"Average text length train NEGATIVE: {np.mean(text_lengths_train_neg)}\")\n",
        "\n",
        "# Define the maximum text length value for flexibility\n",
        "max_text_length = 300\n",
        "num_bins = 100\n",
        "\n",
        "# Filter out text lengths over the defined maximum value\n",
        "filtered_text_lengths_train_pos = [length for length in text_lengths_train_pos if length <= max_text_length]\n",
        "filtered_text_lengths_train_neg = [length for length in text_lengths_train_neg if length <= max_text_length]\n",
        "\n",
        "\n",
        "# Count the number of text lengths over the defined maximum value\n",
        "num_over_max_pos = len(text_lengths_train_pos) - len(filtered_text_lengths_train_pos)\n",
        "num_over_max_neg = len(text_lengths_train_neg) - len(filtered_text_lengths_train_neg)\n",
        "\n",
        "max_length = 256\n",
        "print(f\"Percentage of posts deleted: {100*(num_over_max_neg+num_over_max_pos)/len(text_lengths_train)}%\")\n",
        "\n",
        "# Calculate the number of samples in each array\n",
        "num_samples_pos = len(filtered_text_lengths_train_pos)\n",
        "num_samples_neg = len(filtered_text_lengths_train_neg)\n",
        "\n",
        "# Calculate the normalized histogram for both arrays\n",
        "hist_pos, bins_pos = np.histogram(filtered_text_lengths_train_pos, bins=num_bins, range=(0, max_text_length))\n",
        "hist_neg, bins_neg = np.histogram(filtered_text_lengths_train_neg, bins=num_bins, range=(0, max_text_length))\n",
        "\n",
        "# Convert the histograms to percentages\n",
        "hist_pos_percentage = (hist_pos / num_samples_pos) * 100\n",
        "hist_neg_percentage = (hist_neg / num_samples_neg) * 100\n",
        "\n",
        "# Width of each bar (set to half of the bin width to have bars next to each other)\n",
        "bar_width = (max_text_length / num_bins) / 2\n",
        "\n",
        "# Calculate the x positions for the bars\n",
        "x_pos_pos = bins_pos[:-1]\n",
        "x_pos_neg = bins_neg[:-1] + bar_width\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.bar(x_pos_pos, hist_pos_percentage, width=bar_width, alpha=0.7, label='Depressed', align='center', color='tab:orange')\n",
        "plt.bar(x_pos_neg, hist_neg_percentage, width=bar_width, alpha=0.7, label='Not Depressed', align='center', color='tab:blue')\n",
        "plt.xlabel(\"Post Length (#words)\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.legend()\n",
        "plt.title(str(task))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9OD9r6fm9d7"
      },
      "source": [
        "Analyzing word usage (only for train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqIwESS9noNK"
      },
      "outputs": [],
      "source": [
        "#Emotion Lexicon\n",
        "emotion_lexicon = load_NRC(hyperparams_features['nrc_lexicon_path'])\n",
        "emotions = list(emotion_lexicon.keys())\n",
        "print(emotions)\n",
        "\n",
        "#LIWC Dicttionary\n",
        "liwc_dict = load_LIWC(hyperparams_features['liwc_path'])\n",
        "liwc_categories = set(liwc_dict.keys())\n",
        "print(liwc_categories)\n",
        "\n",
        "#Stopwords\n",
        "stopwords_list = load_stopwords(hyperparams_features['stopwords_path'])\n",
        "print(stopwords_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nuh-8dRQnA2X"
      },
      "outputs": [],
      "source": [
        "max_len = 256\n",
        "\n",
        "total_words_pos = 0\n",
        "total_words_neg = 0\n",
        "\n",
        "total_emo_pos = np.zeros(len(emotions), dtype = int)\n",
        "total_liwc_pos = np.zeros(len(liwc_categories), dtype=int)\n",
        "total_stopwords_pos = np.zeros(len(stopwords_list), dtype = int)\n",
        "\n",
        "total_emo_neg = np.zeros(len(emotions), dtype = int)\n",
        "total_liwc_neg = np.zeros(len(liwc_categories), dtype=int)\n",
        "total_stopwords_neg = np.zeros(len(stopwords_list), dtype = int)\n",
        "\n",
        "\n",
        "for user, user_data in tqdm(user_level_data.items()):\n",
        "  # if total_words_pos>0 and total_words_neg >0:\n",
        "  #   break\n",
        "  #train samples\n",
        "  if len(user) >= 4 and not user[-4:] == '0000':\n",
        "     for text in user_data['texts']:\n",
        "      text = text[:max_len]   #only analyse up to maxlen\n",
        "\n",
        "      encoded_emotions = np.array(encode_emotions(text, emotion_lexicon,emotions, relative = False))\n",
        "      encoded_stopwords = np.array(encode_stopwords(text, stopwords_list, relative = False))\n",
        "      encoded_liwc = np.array(encode_liwc_categories(text, liwc_categories,liwc_dict, relative = False))\n",
        "\n",
        "      #positive\n",
        "      if user_data['label'] ==1:\n",
        "        total_words_pos += len(text)\n",
        "        total_emo_pos += encoded_emotions\n",
        "        total_liwc_pos += encoded_liwc\n",
        "        total_stopwords_pos += encoded_stopwords\n",
        "      #negative\n",
        "      else:\n",
        "        total_words_neg += len(text)\n",
        "        total_emo_neg += encoded_emotions\n",
        "        total_liwc_neg += encoded_liwc\n",
        "        total_stopwords_neg += encoded_stopwords\n",
        "\n",
        "rel_emo_pos = total_emo_pos/total_words_pos\n",
        "rel_liwc_pos = total_liwc_pos/total_words_pos\n",
        "rel_stopwords_pos = total_stopwords_pos/total_words_pos\n",
        "\n",
        "rel_emo_neg = total_emo_neg/total_words_neg\n",
        "rel_liwc_neg = total_liwc_neg/total_words_neg\n",
        "rel_stopwords_neg = total_stopwords_neg/total_words_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6GfNBS_yFVE"
      },
      "outputs": [],
      "source": [
        "#Saving the results to JSONs\n",
        "\n",
        "import json\n",
        "\n",
        "#making dictionaries with emotion/stopwrd/liwc : relative frequency\n",
        "freq_emotions_pos = {}\n",
        "freq_emotions_neg = {}\n",
        "\n",
        "for i, emotion in enumerate(emotions):\n",
        "  freq_emotions_pos[emotion] = rel_emo_pos[i]\n",
        "  freq_emotions_neg[emotion] = rel_emo_neg[i]\n",
        "\n",
        "freq_liwc_pos = {}\n",
        "freq_liwc_neg = {}\n",
        "\n",
        "for i, liwc_cat in enumerate(liwc_categories):\n",
        "  freq_liwc_pos[liwc_cat] = rel_liwc_pos[i]\n",
        "  freq_liwc_neg[liwc_cat] = rel_liwc_neg[i]\n",
        "\n",
        "freq_stopwords_pos = {}\n",
        "freq_stopwords_neg = {}\n",
        "\n",
        "for i, stopwrd in enumerate(stopwords_list):\n",
        "  freq_stopwords_pos[stopwrd] = rel_stopwords_pos[i]\n",
        "  freq_stopwords_neg[stopwrd] = rel_stopwords_neg[i]\n",
        "\n",
        "results = [freq_emotions_neg, freq_emotions_pos, freq_liwc_neg, freq_liwc_pos, freq_stopwords_neg, freq_stopwords_pos]\n",
        "names_results = [\"freq_emotions_neg\", \"freq_emotions_pos\", \"freq_liwc_neg\", \"freq_liwc_pos\", \"freq_stopwords_neg\", \"freq_stopwords_pos\"]\n",
        "\n",
        "#saving the dicts as json:\n",
        "\n",
        "save_path_root = root_dir + \"/Data Analysis/\" + task\n",
        "\n",
        "for i, results_dict in enumerate(results):\n",
        "\n",
        "  save_path = save_path_root + \"/\" + names_results[i] + \".json\"\n",
        "\n",
        "  with open(save_path, \"w\") as file:\n",
        "    json.dump(results_dict, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qImwTjflvaHn"
      },
      "outputs": [],
      "source": [
        "#analyzing the JSON saved results\n",
        "\n",
        "#for task in [\"Self-Harm, \"Drpression\", \"Anorexia\"]:\n",
        "import json\n",
        "save_path_root = root_dir + \"/Data Analysis/\" + task + \"/\"\n",
        "\n",
        "with open(save_path_root + \"freq_emotions_neg.json\") as file:\n",
        "  freq_emotions_neg = json.load(file)\n",
        "\n",
        "with open(save_path_root + \"freq_emotions_pos.json\") as file:\n",
        "  freq_emotions_pos = json.load(file)\n",
        "\n",
        "with open(save_path_root + \"freq_liwc_neg.json\") as file:\n",
        "  freq_liwc_neg = json.load(file)\n",
        "\n",
        "with open(save_path_root + \"freq_liwc_pos.json\") as file:\n",
        "  freq_liwc_pos = json.load(file)\n",
        "\n",
        "with open(save_path_root + \"freq_stopwords_neg.json\") as file:\n",
        "  freq_stopwords_neg = json.load(file)\n",
        "\n",
        "with open(save_path_root + \"freq_stopwords_pos.json\") as file:\n",
        "  freq_stopwords_pos = json.load(file)\n",
        "\n",
        "negative_pos = freq_emotions_pos['negative']\n",
        "negative_neg = freq_emotions_neg['negative']\n",
        "\n",
        "positive_pos = freq_emotions_pos['positive']\n",
        "positive_neg = freq_emotions_neg['positive']\n",
        "\n",
        "sadness_pos = freq_emotions_pos['sadness']\n",
        "sadness_neg = freq_emotions_neg['sadness']\n",
        "\n",
        "health_pos = freq_liwc_pos['health']\n",
        "health_neg = freq_liwc_neg['health']\n",
        "\n",
        "personal_pos = (freq_stopwords_pos['i'] + freq_stopwords_pos['me'] + freq_stopwords_pos['my'] +freq_stopwords_pos['myself'])/4\n",
        "personal_neg = (freq_stopwords_neg['i'] + freq_stopwords_neg['me'] + freq_stopwords_neg['my'] +freq_stopwords_neg['myself'])/4\n",
        "\n",
        "disgust_pos = freq_emotions_pos['disgust']\n",
        "disgust_neg = freq_emotions_neg['disgust']\n",
        "\n",
        "friend_pos = freq_liwc_pos['friend']\n",
        "friend_neg = freq_liwc_neg['friend']\n",
        "\n",
        "\n",
        "print(f\"People with {task} talk {100*(negative_pos-negative_neg)/negative_neg}% more negative.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(positive_pos-positive_neg)/positive_neg}% more positive.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(sadness_pos-sadness_neg)/sadness_neg}% more sadness.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(health_pos-health_neg)/health_neg}% more health.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(personal_pos-personal_neg)/personal_neg}% more personal.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(disgust_pos-disgust_neg)/disgust_neg}% more disgust.\")\n",
        "\n",
        "print(f\"People with {task} talk {100*(friend_pos-friend_neg)/friend_neg}% more friend.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9LqQTDtFdKd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Replace the percentages below with your actual data\n",
        "pos = [negative_pos, positive_pos, personal_pos, sadness_pos, health_pos, disgust_pos, friend_pos]\n",
        "pos_pct = [100*cat for cat in pos]\n",
        "neg = [negative_neg, positive_neg, personal_neg, sadness_neg, health_neg, disgust_neg, friend_neg]\n",
        "neg_pct = [100*cat for cat in neg]\n",
        "labels = ['Negative', 'Positive', 'Personal', 'Sadness', 'Health', 'Disgust', 'Friend']\n",
        "\n",
        "\n",
        "# Calculate the number of pairs and set the width of the bars\n",
        "num_pairs = len(labels)\n",
        "bar_width = 0.35\n",
        "\n",
        "# Create an array of positions for the bars\n",
        "x = np.arange(num_pairs)\n",
        "\n",
        "#plt.figure(figsize=(10, 30))\n",
        "\n",
        "# Create the positive and negative bars\n",
        "plt.bar(x+ bar_width, pos_pct, width=bar_width, color='tab:orange', label='Depressed')\n",
        "plt.bar(x, neg_pct, width=bar_width, color='tab:blue', label='Not Depressed')\n",
        "\n",
        "# Calculate percentage difference for each pair\n",
        "percentage_diff = [(pos - neg) / neg * 100 for pos, neg in zip(pos_pct, neg_pct)]\n",
        "\n",
        "# Add percentage difference text above each pair of bars in italics\n",
        "for i, diff in enumerate(percentage_diff):\n",
        "    plt.text(x[i] + bar_width / 2, max(pos_pct[i], neg_pct[i]) + 0.2, f'{diff:.1f}%', ha='center', fontstyle='italic')\n",
        "\n",
        "\n",
        "\n",
        "# Add labels, title, and legend\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Percentage of all words')\n",
        "plt.title(f\"{task}\")\n",
        "plt.xticks(x + bar_width / 2, labels)\n",
        "plt.legend()\n",
        "\n",
        "# Adjust the upper limit of the y-axis to make space for the text above the highest bar\n",
        "plt.ylim(0, max(max(pos_pct), max(neg_pct)) + 1)  # Increase the '10' to provide more space if needed\n",
        "\n",
        "\n",
        "# Add whitespace between the bars\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in6c8sLcwFts"
      },
      "outputs": [],
      "source": [
        "import statistics as stat\n",
        "min_diff=100\n",
        "\n",
        "total_diff = 0\n",
        "all_diff = []\n",
        "\n",
        "total_cats=0\n",
        "\n",
        "for cat, percent in freq_emotions_pos.items():\n",
        "  if freq_emotions_neg[cat]>0:\n",
        "    diff = 100*(percent - freq_emotions_neg[cat])/freq_emotions_neg[cat]\n",
        "    total_cats +=1\n",
        "    total_diff+=abs(diff)\n",
        "    all_diff.append(abs(diff))\n",
        "  else:\n",
        "    diff = percent\n",
        "\n",
        "  if abs(diff) > min_diff:\n",
        "    print(f\"In emotions, people with {task} talk {diff} more about {cat}\")\n",
        "\n",
        "\n",
        "\n",
        "for cat, percent in freq_liwc_pos.items():\n",
        "  if freq_liwc_neg[cat]>0:\n",
        "    diff = 100*(percent - freq_liwc_neg[cat])/freq_liwc_neg[cat]\n",
        "    total_cats +=1\n",
        "    total_diff+=abs(diff)\n",
        "    all_diff.append(abs(diff))\n",
        "  else:\n",
        "    diff = percent\n",
        "\n",
        "  if abs(diff) > min_diff:\n",
        "    print(f\"In LIWC, people with {task} talk {diff} more about {cat}\")\n",
        "\n",
        "  total_cats +=1\n",
        "  total_diff+=abs(diff)\n",
        "\n",
        "for cat, percent in freq_stopwords_pos.items():\n",
        "  if freq_stopwords_neg[cat]>0:\n",
        "    diff = 100*(percent - freq_stopwords_neg[cat])/freq_stopwords_neg[cat]\n",
        "    total_cats +=1\n",
        "    total_diff+=abs(diff)\n",
        "    all_diff.append(abs(diff))\n",
        "  else:\n",
        "    diff = percent\n",
        "\n",
        "  total_cats +=1\n",
        "  total_diff+=abs(diff)\n",
        "\n",
        "  if abs(diff) > min_diff:\n",
        "    print(f\"In stopwords, people with {task} talk {diff} more about {cat}\")\n",
        "\n",
        "print(f\"Average absolute difference across all categories for {task} is {total_diff/total_cats}%.\")\n",
        "print(f\"Median absolute difference across all categories for {task} is {stat.median(all_diff)}%.\")\n",
        "\n",
        "print(stat.mean(all_diff))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQs9JzNeCrgg"
      },
      "source": [
        "# Analyzing Attention Scores for Results Chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPw2SQFSBLWl"
      },
      "source": [
        "Analyzing User-level attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQrgUaxvCykH"
      },
      "outputs": [],
      "source": [
        "#root_dir = \"/Users/ronhochstenbach/Desktop/Thesis/Data\"\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"  #when cloning for colab\n",
        "\n",
        "\n",
        "hyperparams_features = {\n",
        "    \"max_features\": 20002,\n",
        "    \"embedding_dim\": 300,\n",
        "    \"vocabulary_path\": root_dir + '/Resources/vocabulary.pkl',\n",
        "    \"nrc_lexicon_path\" : root_dir + \"/Resources/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\",\n",
        "    \"liwc_path\": root_dir + '/Resources/LIWC2007.dic',\n",
        "    \"stopwords_path\": root_dir + '/Resources/stopwords.txt',\n",
        "    \"embeddings_path\": root_dir + \"/Resources/glove.840B.300d.txt\",\n",
        "    #\"liwc_words_cached\": \"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\"\n",
        "    \"BERT_path\": root_dir + '/Resources/BERT-base-uncased/'\n",
        "}\n",
        "\n",
        "hyperparams = {\n",
        "    \"trainable_embeddings\": True,\n",
        "    \"sum_layers\": 1,\n",
        "    'trainable_bert_layer': False,\n",
        "\n",
        "    #Structurel\n",
        "    \"lstm_units\": 128,\n",
        "    \"dense_bow_units\": 20,\n",
        "    \"dense_numerical_units\": 20,\n",
        "    \"lstm_units_user\": 32,\n",
        "\n",
        "    #Self-attention structure\n",
        "    \"num_heads\": 3,\n",
        "    \"key_dim\": 150,\n",
        "    \"num_layers\":2,\n",
        "    \"use_positional_encodings\": True,\n",
        "\n",
        "    #Regularizers\n",
        "    \"dropout\": 0.3,             #Appendix uban\n",
        "    \"l2_dense\": 0.00001,        #Appendix uban (?)\n",
        "    \"l2_embeddings\": 0.00001,   #Appendix uban (?)\n",
        "    \"norm_momentum\": 0.1,\n",
        "\n",
        "    \"ignore_layer\": [\"bert_layer\"],\n",
        "\n",
        "    #Training\n",
        "    \"decay\": 0.001,\n",
        "    \"lr\": 0.0005,                   #appendix uban 0.0001 (han etc, 0.0005 hsan)\n",
        "    \"reduce_lr_factor\": 0.5,        #originally 0.5\n",
        "    \"reduce_lr_patience\": 55,        #originally 55\n",
        "    \"scheduled_reduce_lr_freq\": 95,  #originally: 95\n",
        "    \"scheduled_reduce_lr_factor\": 0.5,\n",
        "    \"freeze_patience\": 2000,\n",
        "    \"threshold\": 0.5,\n",
        "    \"early_stopping_patience\": 5,\n",
        "\n",
        "    \"positive_class_weight\": 2,     #6.5 = calculated, 2 = uban history & own hyperopt\n",
        "\n",
        "    \"maxlen\": 256,\n",
        "    \"posts_per_user\": None,\n",
        "    \"post_groups_per_user\": None,\n",
        "    \"posts_per_group\": 50,\n",
        "    \"batch_size\": 32,   #normally 32\n",
        "    \"padding\": \"pre\",\n",
        "    \"hierarchical\": True,\n",
        "    \"sample_seqs\": False,\n",
        "    \"sampling_distr\": \"exp\",\n",
        "\n",
        "}\n",
        "\n",
        "hyperparams['optimizer'] = optimizers.legacy.Adam(learning_rate=hyperparams['lr'],\n",
        "                                                  decay = hyperparams['decay'])\n",
        "\n",
        "# with open('/Users/ronhochstenbach/Desktop/Thesis/Data/Resources/config.json', 'w') as file:\n",
        "#     json.dump(hyperparams_features, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE756SHJEP6X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from load_save_model import load_saved_model_weights, load_params\n",
        "from data_generator import DataGenerator_Base, DataGenerator_BERT\n",
        "from data_loader import load_erisk_data\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"  #when cloning for colab\n",
        "saved_path = root_dir + '/Final Trained Models (10 epochs)/Depression/Depression_HAN_2023-07-31 23:35:39.671308'\n",
        "#hyperparams, hyperparams_features = load_params(saved_path, general_config_path=\"/content/drive/MyDrive/Thesis/Data/Resources/config.json\")\n",
        "\n",
        "task = \"Self-Harm\"          #\"Self-Harm\" - \"Anorexia\" - \"Depression\"\n",
        "model_type = \"HAN\"          #\"HAN\" - \"HAN_BERT\"\n",
        "print(f\"Running {task} task using the {model_type} model!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9--6T5gExrU"
      },
      "outputs": [],
      "source": [
        "task = \"Depression\"\n",
        "\n",
        "if task == \"Self-Harm\":\n",
        "  writings_df = writings_df_sh\n",
        "  user_level_data = user_level_data_sh\n",
        "  subjects_split = subjects_split_sh\n",
        "elif task == \"Anorexia\":\n",
        "  writings_df = writings_df_a\n",
        "  user_level_data = user_level_data_a\n",
        "  subjects_split = subjects_split_a\n",
        "elif task == \"Depression\":\n",
        "  writings_df = writings_df_d\n",
        "  user_level_data = user_level_data_d\n",
        "  subjects_split = subjects_split_d\n",
        "else:\n",
        "  raise Exception(\"Unkown data set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnM58d8vGAqE"
      },
      "source": [
        "Importing model, initializing datagenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB25gxLiF-jI"
      },
      "outputs": [],
      "source": [
        "model = load_saved_model_weights(saved_path, hyperparams, hyperparams_features, model_type, h5=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLZFElgLGMIq"
      },
      "outputs": [],
      "source": [
        "if model_type == \"HAN\" or model_type == \"HSAN\":\n",
        "    data_gen_class = DataGenerator_Base\n",
        "    data_generator_train = DataGenerator_Base(user_level_data, subjects_split, set_type='train',\n",
        "                                              hyperparams_features=hyperparams_features,\n",
        "                                              seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
        "                                              posts_per_group=hyperparams['posts_per_group'], post_groups_per_user=None,\n",
        "                                              max_posts_per_user=hyperparams['posts_per_user'],\n",
        "                                              compute_liwc=True,\n",
        "                                              ablate_emotions='emotions' in hyperparams['ignore_layer'],\n",
        "                                              ablate_liwc='liwc' in hyperparams['ignore_layer'])\n",
        "elif model_type == \"HAN_BERT\" or model_type == \"Con_HAN\":\n",
        "    data_gen_class = DataGenerator_BERT\n",
        "    data_generator_train = DataGenerator_BERT(user_level_data, subjects_split, set_type='train',\n",
        "                                              hyperparams_features=hyperparams_features, model_type=model_type,\n",
        "                                              seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
        "                                              posts_per_group=hyperparams['posts_per_group'],\n",
        "                                              post_groups_per_user=None,\n",
        "                                              max_posts_per_user=hyperparams['posts_per_user'],\n",
        "                                              compute_liwc=True,\n",
        "                                              ablate_emotions='emotions' in hyperparams['ignore_layer'],\n",
        "                                              ablate_liwc='liwc' in hyperparams['ignore_layer'])\n",
        "else:\n",
        "    raise Exception(\"Unknown type!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJQ82mVy4mrI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "if not model_type == \"HAN_BERT\":\n",
        "  raise Exception(\"Use Bert for this!\")\n",
        "\n",
        "min_pos_chunks = 200\n",
        "\n",
        "length_attn_pairs = []\n",
        "\n",
        "total_chunk_attention_pos = np.zeros(50)\n",
        "total_chunk_pos = 0\n",
        "\n",
        "total_chunk_attention_neg = np.zeros(50)\n",
        "total_chunk_neg = 0\n",
        "\n",
        "for i, (x,y) in enumerate(tqdm(data_generator_train)):\n",
        "  #Obtain post attention weights for the batch\n",
        "  attention_layer = model.get_layer(\"attention_user\")\n",
        "  attention_function = tf.keras.backend.function(inputs=[model.input], outputs=[attention_layer.output])\n",
        "  predictions = model.predict(x, verbose=False)\n",
        "  attention_weights = np.squeeze(attention_function(x)[0])\n",
        "\n",
        "  batch_tokens = x[0]\n",
        "\n",
        "  for c in range(batch_tokens.shape[0]):    #in 32\n",
        "    chunk_attentions = attention_weights[c]\n",
        "    label = int(y[c])\n",
        "\n",
        "    if label == 1:  #positive\n",
        "      total_chunk_attention_pos += chunk_attentions\n",
        "      total_chunk_pos += 1\n",
        "\n",
        "    if label == 0:  #negative\n",
        "      total_chunk_attention_neg += chunk_attentions\n",
        "      total_chunk_neg +=1\n",
        "\n",
        "    for p in range(batch_tokens.shape[1]):          #in 50\n",
        "\n",
        "      length = np.count_nonzero(batch_tokens[c,p]) - 2 #Subtract special tokens\n",
        "      post_attention = chunk_attentions[p]\n",
        "\n",
        "      if length>0:\n",
        "        length_attn_pairs.append([length, post_attention])\n",
        "\n",
        "  print(f\"After {i+1} batches, we have {total_chunk_pos} positive chunks, {total_chunk_neg} negative chunks, and {len(length_attn_pairs)} len/attn pairs!\")\n",
        "\n",
        "  #if total_chunk_pos >= min_pos_chunks:\n",
        "  #  break\n",
        "\n",
        "avg_chunk_att_pos = total_chunk_attention_pos / total_chunk_pos\n",
        "avg_chunk_att_neg = total_chunk_attention_neg / total_chunk_neg\n",
        "\n",
        "#saving to drive\n",
        "name_chunck_att_pos = root_dir + \"/Data Analysis/\" + task + \"/chunk_att_pos.csv\"\n",
        "name_chunck_att_neg = root_dir + \"/Data Analysis/\" + task + \"/chunk_att_neg.csv\"\n",
        "name_length_att = root_dir + \"/Data Analysis/\" + task + \"/name_length_att.csv\"\n",
        "\n",
        "np.savetxt(name_chunck_att_pos, avg_chunk_att_pos, delimiter=\",\")\n",
        "np.savetxt(name_chunck_att_neg, avg_chunk_att_neg, delimiter=\",\")\n",
        "\n",
        "with open(name_length_att, 'w', newline='') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_writer.writerows(length_attn_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting user-level attention"
      ],
      "metadata": {
        "id": "G8UagEnwFx89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ade32mFakTt"
      },
      "outputs": [],
      "source": [
        "task = \"Anorexia\"\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"\n",
        "\n",
        "att_series_pos = np.genfromtxt(root_dir + '/Data Analysis/'+ task + '/chunk_att_pos.csv', delimiter=',')\n",
        "att_series_pos = np.exp(att_series_pos)\n",
        "att_series_pos = att_series_pos/sum(att_series_pos)\n",
        "\n",
        "att_series_neg = np.genfromtxt(root_dir + '/Data Analysis/'+ task + '/chunk_att_neg.csv', delimiter=',')\n",
        "att_series_neg = np.exp(att_series_neg)\n",
        "att_series_neg = att_series_neg / sum(att_series_neg)\n",
        "\n",
        "\n",
        "x_values = np.arange(1, 51)\n",
        "plt.plot(x_values, att_series_pos, label='Anorexic', color='tab:orange')\n",
        "plt.plot(x_values, att_series_neg, label='Not Anorexic', color = 'tab:blue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Post')\n",
        "plt.ylabel('Attention Score')\n",
        "plt.title(task)\n",
        "\n",
        "# Add a legend to distinguish the two arrays\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlating user-level attention with post length"
      ],
      "metadata": {
        "id": "Nzg-7PTHDuWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_data(len_att, num_bins=256, binsize=100, replace = False):\n",
        "    lens = len_att[:, 0]\n",
        "    atts = len_att[:, 1]\n",
        "\n",
        "    # Calculate the histogram to divide the data into bins\n",
        "    hist, bin_edges = np.histogram(lens, bins=num_bins)\n",
        "\n",
        "    # Initialize arrays to store the resampled data\n",
        "    resampled_data = np.zeros((num_bins * binsize, 2))\n",
        "\n",
        "    for i in range(num_bins):\n",
        "        # Identify the data points within the current bin\n",
        "        mask = (lens >= bin_edges[i]) & (lens < bin_edges[i + 1])\n",
        "        bin_data = len_att[mask]\n",
        "\n",
        "        if len(bin_data) >= binsize:\n",
        "            # Randomly sample points from the current bin\n",
        "            bin_indices = np.random.choice(len(bin_data), binsize, replace=False)\n",
        "        else:\n",
        "          if replace:\n",
        "            # If the current bin has fewer data points than binsize, sample with replacement\n",
        "            bin_indices = np.random.choice(len(bin_data), binsize, replace=True)\n",
        "          else:\n",
        "            raise Exception(\"samples with replacement\")\n",
        "\n",
        "        # Select the data points from the current bin\n",
        "        bin_data = bin_data[bin_indices]\n",
        "\n",
        "        # Assign the selected data points to the resampled array\n",
        "        resampled_data[i * binsize: (i + 1) * binsize] = bin_data\n",
        "\n",
        "    return resampled_data"
      ],
      "metadata": {
        "id": "4-bMkXYYqcVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"\n",
        "\n",
        "bins = 25\n",
        "binsize = 150\n",
        "replace = False\n",
        "\n",
        "# Create a 1x3 grid for subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for i, task in enumerate([\"Self-Harm\", \"Anorexia\", \"Depression\"]):\n",
        "    len_att = np.genfromtxt(root_dir + '/Data Analysis/'+ task + '/name_length_att.csv', delimiter=',')\n",
        "\n",
        "    #scaling attns\n",
        "    len_att[:,1] = np.exp(len_att[:,1])\n",
        "    len_att[:,1] = len_att[:,1]/(sum(len_att[:,1])*50/len(len_att[:,1]))\n",
        "\n",
        "\n",
        "    new = resample_data(len_att, num_bins=bins, binsize=binsize, replace=replace)\n",
        "\n",
        "    # Plot the hexbin plot in the corresponding subplot\n",
        "    hb = axes[i].hexbin(new[:, 0], new[:, 1], gridsize=bins, cmap='Blues', mincnt=1)\n",
        "\n",
        "    # Add labels and title to the subplot\n",
        "    if i==1:\n",
        "      axes[i].set_xlabel('Post Length')\n",
        "    if i==0:\n",
        "      axes[i].set_ylabel('Attention Score')\n",
        "    axes[i].set_title(task)\n",
        "\n",
        "    # Show the colorbar to indicate the density\n",
        "    if i==2:\n",
        "      cb = fig.colorbar(hb, ax=axes[i])\n",
        "      cb.set_label('Density')\n",
        "\n",
        "# Adjust spacing between subplots and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YPHmt4sHDtrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-level attention"
      ],
      "metadata": {
        "id": "fk1NtDoDlVVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Emotion Lexicon\n",
        "emotion_lexicon = load_NRC(hyperparams_features['nrc_lexicon_path'])\n",
        "emotions = list(emotion_lexicon.keys())\n",
        "print(emotions)\n",
        "\n",
        "#LIWC Dicttionary\n",
        "liwc_dict = load_LIWC(hyperparams_features['liwc_path'])\n",
        "liwc_categories = set(liwc_dict.keys())\n",
        "print(liwc_categories)\n",
        "\n",
        "#Stopwords\n",
        "stopwords_list = load_stopwords(hyperparams_features['stopwords_path'])\n",
        "print(stopwords_list)\n",
        "\n",
        "pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"]\n",
        "\n",
        "vocabulary = load_vocabulary(hyperparams_features['vocabulary_path'])"
      ],
      "metadata": {
        "id": "E2j9vej6t0f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapt inititialize models to pass sentEncoder"
      ],
      "metadata": {
        "id": "F3yHQUsog2Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
        "    Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute, MultiHeadAttention\n",
        "from keras_nlp.layers import SinePositionEncoding\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.metrics import AUC, Precision, Recall\n",
        "from metrics import Metrics\n",
        "from resource_loader import load_embeddings\n",
        "from transformers import TFBertModel, TFRobertaModel, TFAlbertModel\n",
        "from auxilliary_functions import create_embeddings\n",
        "\n",
        "def build_HAN(hyperparams, hyperparams_features,\n",
        "                             emotions_dim, stopwords_list_dim, liwc_categories_dim,\n",
        "                             ignore_layer=[]):\n",
        "\n",
        "    embedding_matrix = load_embeddings(hyperparams_features['embeddings_path'],\n",
        "                                       hyperparams_features['embedding_dim'],\n",
        "                                       hyperparams_features['vocabulary_path'])\n",
        "\n",
        "    # Post/sentence representation - word sequence\n",
        "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
        "    embedding_layer = Embedding(hyperparams_features['max_features'],\n",
        "                                hyperparams_features['embedding_dim'],\n",
        "                                input_length=hyperparams['maxlen'],\n",
        "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=hyperparams['trainable_embeddings'],\n",
        "                                name='embeddings_layer')(tokens_features)\n",
        "\n",
        "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
        "\n",
        "    lstm_layers = LSTM(hyperparams['lstm_units'],\n",
        "                       return_sequences='attention' not in ignore_layer,\n",
        "                       name='LSTM_layer')(embedding_layer)\n",
        "\n",
        "    # Attention\n",
        "    if 'attention' not in ignore_layer:\n",
        "        attention_layer = Dense(1, activation='tanh', name='attention')\n",
        "        attention = attention_layer(lstm_layers)\n",
        "        attention = Flatten()(attention)\n",
        "        attention_output = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(hyperparams['lstm_units'])(attention_output)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = Multiply()([lstm_layers, attention])\n",
        "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1),\n",
        "                                     output_shape=(hyperparams['lstm_units'],)\n",
        "                                     )(sent_representation)\n",
        "    else:\n",
        "        sent_representation = lstm_layers\n",
        "\n",
        "    if 'batchnorm' not in ignore_layer:\n",
        "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
        "                                                 name='sent_repr_norm')(sent_representation)\n",
        "\n",
        "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
        "\n",
        "    # Other features\n",
        "    numerical_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        emotions_dim + 1 + liwc_categories_dim\n",
        "    ), name='numeric_input_hist')  # emotions and pronouns\n",
        "    sparse_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        stopwords_list_dim\n",
        "    ), name='sparse_input_hist')  # stopwords\n",
        "\n",
        "    posts_history_input = Input(shape=(hyperparams['posts_per_group'],\n",
        "                                       hyperparams['maxlen']\n",
        "                                       ), name='hierarchical_word_seq_input')\n",
        "\n",
        "    # Hierarchy\n",
        "    sentEncoder = Model(inputs=tokens_features,\n",
        "                        outputs=sent_representation, name='sentEncoder')\n",
        "    sentEncoder.summary()\n",
        "\n",
        "    sentEncoder.compile(hyperparams['optimizer'], K.binary_crossentropy,\n",
        "                               metrics=[AUC(), Precision(), Recall()])\n",
        "\n",
        "\n",
        "    return sentEncoder\n",
        "\n",
        "def build_HAN_BERT(hyperparams, hyperparams_features, model_type,\n",
        "                             emotions_dim, stopwords_list_dim, liwc_categories_dim,\n",
        "                             ignore_layer=[]):\n",
        "\n",
        "    # Post/sentence representation - word sequence\n",
        "    tokens_features_ids = Input(shape=(hyperparams['maxlen'],), name='word_seq_ids',dtype=tf.int32)\n",
        "    tokens_features_attnmasks = Input(shape=(hyperparams['maxlen'],), name='word_seq_attnmasks',dtype=tf.int32)\n",
        "\n",
        "    #extracting the last four hidden states and summing them\n",
        "    if model_type == \"HAN_BERT\":\n",
        "        # BERT_embedding_layer = TFBertModel.from_pretrained('bert-base-uncased')(\n",
        "        #                                                     tokens_features_ids, attention_mask=tokens_features_attnmasks,\n",
        "        #                                                     output_hidden_states=True, return_dict=True)[\n",
        "        #                                                                            'hidden_states'][-4:]\n",
        "        BERT_embedding_layer = TFBertModel.from_pretrained(\"prajjwal1/bert-tiny\", from_pt=True)(\n",
        "                                                            tokens_features_ids, attention_mask=tokens_features_attnmasks,\n",
        "                                                            output_hidden_states=True, return_dict=True)[\n",
        "                                                                                   'hidden_states'][-hyperparams['sum_layers']:]\n",
        "        # BERT_embedding_layer = TFAlbertModel.from_pretrained(\"albert-base-v2\", from_pt=True)(\n",
        "        #                                                 tokens_features_ids, attention_mask=tokens_features_attnmasks,\n",
        "        #                                                 output_hidden_states=True, return_dict=True)[\n",
        "        #                                                                        'hidden_states'][-hyperparams['sum_layers']:]\n",
        "\n",
        "    elif model_type == \"HAN_RoBERTa\":\n",
        "        BERT_embedding_layer = TFRobertaModel.from_pretrained('roberta-base')(\n",
        "                                                            tokens_features_ids, attention_mask=tokens_features_attnmasks,\n",
        "                                                            output_hidden_states=True, return_dict=True)[\n",
        "                                                                                   'hidden_states'][-4:]\n",
        "    else:\n",
        "        Exception(\"Unknown model type!\")\n",
        "\n",
        "    #embedding_layer = Lambda(lambda x: tf.add_n([layer for layer in x]))(BERT_embedding_layer)\n",
        "    embedding_layer = Lambda(lambda x: tf.add_n(x))(BERT_embedding_layer)\n",
        "\n",
        "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
        "\n",
        "    lstm_layers = LSTM(hyperparams['lstm_units'],\n",
        "                       return_sequences='attention' not in ignore_layer,\n",
        "                       name='LSTM_layer')(embedding_layer)\n",
        "\n",
        "    # Attention\n",
        "    if 'attention' not in ignore_layer:\n",
        "        attention_layer = Dense(1, activation='tanh', name='attention')\n",
        "        attention = attention_layer(lstm_layers)\n",
        "        attention = Flatten()(attention)\n",
        "        attention_output = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(hyperparams['lstm_units'])(attention_output)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = Multiply()([lstm_layers, attention])\n",
        "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1),\n",
        "                                     output_shape=(hyperparams['lstm_units'],)\n",
        "                                     )(sent_representation)\n",
        "    else:\n",
        "        sent_representation = lstm_layers\n",
        "\n",
        "    if 'batchnorm' not in ignore_layer:\n",
        "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
        "                                                 name='sent_repr_norm')(sent_representation)\n",
        "\n",
        "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
        "\n",
        "    # Other features\n",
        "    numerical_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        emotions_dim + 1 + liwc_categories_dim\n",
        "    ), name='numeric_input_hist')  # emotions and pronouns\n",
        "    sparse_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        stopwords_list_dim\n",
        "    ), name='sparse_input_hist')  # stopwords\n",
        "\n",
        "    post_history_ids = Input(shape=(hyperparams['posts_per_group'],\n",
        "                                       hyperparams['maxlen']\n",
        "                                       ), name='hierarchical_word_seq_input_ids')\n",
        "    post_history_attnmasks = Input(shape=(hyperparams['posts_per_group'],\n",
        "                                       hyperparams['maxlen']\n",
        "                                       ), name='hierarchical_word_seq_input_attnmasks')\n",
        "\n",
        "    # Hierarchy\n",
        "    sentEncoder = Model(inputs=[tokens_features_ids,tokens_features_attnmasks],\n",
        "                        outputs=sent_representation, name='sentEncoder')\n",
        "\n",
        "    sentEncoder.summary()\n",
        "\n",
        "    sentEncoder.compile(hyperparams['optimizer'], K.binary_crossentropy,\n",
        "                            metrics=[AUC(), Precision(), Recall()])\n",
        "\n",
        "    return sentEncoder\n",
        "\n",
        "def build_Context_HAN(hyperparams, hyperparams_features,\n",
        "                             emotions_dim, stopwords_list_dim, liwc_categories_dim,\n",
        "                             ignore_layer=[]):\n",
        "\n",
        "    # Post/sentence representation - word sequence\n",
        "    tokens_features_ids = Input(shape=(hyperparams['maxlen'],), name='word_seq_ids',dtype=tf.int32)\n",
        "    tokens_features_attnmasks = Input(shape=(hyperparams['maxlen'],), name='word_seq_attnmasks',dtype=tf.int32)\n",
        "\n",
        "    #extracting the last four hidden states and summing them\n",
        "    BERT_embedding_layer = TFBertModel.from_pretrained(\"prajjwal1/bert-tiny\", from_pt=True)(\n",
        "                                                        tokens_features_ids, attention_mask=tokens_features_attnmasks,\n",
        "                                                        output_hidden_states=True, return_dict=True)[\n",
        "                                                                               'hidden_states'][-hyperparams['sum_layers']:]\n",
        "\n",
        "    #embedding_layer = Lambda(lambda x: tf.add_n([layer for layer in x]))(BERT_embedding_layer)\n",
        "    embedding_layer = Lambda(lambda x: tf.add_n(x))(BERT_embedding_layer)\n",
        "\n",
        "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
        "\n",
        "    lstm_layers = LSTM(hyperparams['lstm_units'],\n",
        "                       return_sequences='attention' not in ignore_layer,\n",
        "                       name='LSTM_layer')(embedding_layer)\n",
        "\n",
        "    # Attention\n",
        "    if 'attention' not in ignore_layer:\n",
        "        attention_layer = Dense(1, activation='tanh', name='attention')\n",
        "        attention = attention_layer(lstm_layers)\n",
        "        attention = Flatten()(attention)\n",
        "        attention_output = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(hyperparams['lstm_units'])(attention_output)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = Multiply()([lstm_layers, attention])\n",
        "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1),\n",
        "                                     output_shape=(hyperparams['lstm_units'],)\n",
        "                                     )(sent_representation)\n",
        "    else:\n",
        "        sent_representation = lstm_layers\n",
        "\n",
        "    if 'batchnorm' not in ignore_layer:\n",
        "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
        "                                                 name='sent_repr_norm')(sent_representation)\n",
        "\n",
        "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
        "\n",
        "    # Other features\n",
        "    numerical_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        emotions_dim + 1 + liwc_categories_dim\n",
        "    ), name='numeric_input_hist')  # emotions and pronouns\n",
        "    sparse_features_history = Input(shape=(\n",
        "        hyperparams['posts_per_group'],\n",
        "        stopwords_list_dim\n",
        "    ), name='sparse_input_hist')  # stopwords\n",
        "\n",
        "    post_history_ids = Input(shape=(hyperparams['posts_per_group'],\n",
        "                                       hyperparams['maxlen']\n",
        "                                       ), name='hierarchical_word_seq_input_ids')\n",
        "    post_history_attnmasks = Input(shape=(hyperparams['posts_per_group'],\n",
        "                                       hyperparams['maxlen']\n",
        "                                       ), name='hierarchical_word_seq_input_attnmasks')\n",
        "\n",
        "    # Hierarchy\n",
        "    sentEncoder = Model(inputs=[tokens_features_ids,tokens_features_attnmasks],\n",
        "                        outputs=sent_representation, name='sentEncoder')\n",
        "\n",
        "\n",
        "    sentEncoder.compile(hyperparams['optimizer'], K.binary_crossentropy,\n",
        "                            metrics=[AUC(), Precision(), Recall()])\n",
        "\n",
        "    return sentEncoder"
      ],
      "metadata": {
        "id": "rfNtEkPVh_CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(hyperparams, hyperparams_features, model_type,\n",
        "                     logger=None, session=None, transfer=False):\n",
        "    if not logger:\n",
        "        logger = logging.getLogger('training')\n",
        "        ch = logging.StreamHandler(sys.stdout)\n",
        "        # create formatter\n",
        "        formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
        "        # add formatter to ch\n",
        "        ch.setFormatter(formatter)\n",
        "        # add ch to logger\n",
        "        logger.addHandler(ch)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "    logger.info(\"Initializing model...\\n\")\n",
        "    if 'emotions' in hyperparams['ignore_layer']:\n",
        "        emotions_dim = 0\n",
        "    else:\n",
        "        emotions = load_NRC(hyperparams_features['nrc_lexicon_path'])\n",
        "        emotions_dim = len(emotions)\n",
        "    if 'liwc' in hyperparams['ignore_layer']:\n",
        "        liwc_categories_dim = 0\n",
        "    else:\n",
        "        liwc_categories = load_LIWC(hyperparams_features['liwc_path'])\n",
        "        liwc_categories_dim = len(liwc_categories)\n",
        "    if 'stopwords' in hyperparams['ignore_layer']:\n",
        "        stopwords_dim = 0\n",
        "    else:\n",
        "        stopwords_list = load_stopwords(hyperparams_features['stopwords_path'])\n",
        "        stopwords_dim = len(stopwords_list)\n",
        "\n",
        "    # Initialize model\n",
        "\n",
        "    if model_type == 'HAN':\n",
        "        sentEncoder = build_HAN(hyperparams, hyperparams_features,\n",
        "                                         emotions_dim, stopwords_dim, liwc_categories_dim,\n",
        "                                         ignore_layer=hyperparams['ignore_layer'])\n",
        "    elif model_type == 'HAN_BERT' or model_type == \"HAN_RoBERTa\":\n",
        "        sentEncoder = build_HAN_BERT(hyperparams, hyperparams_features, model_type,\n",
        "                                         emotions_dim, stopwords_dim, liwc_categories_dim,\n",
        "                                         ignore_layer=hyperparams['ignore_layer'])\n",
        "    elif model_type == 'Con_HAN':\n",
        "        sentEncoder = build_Context_HAN(hyperparams, hyperparams_features,\n",
        "                                          emotions_dim, stopwords_dim, liwc_categories_dim,\n",
        "                                          ignore_layer=hyperparams['ignore_layer'])\n",
        "    else:\n",
        "        raise Exception(\"Unknown model!\")\n",
        "\n",
        "    return sentEncoder"
      ],
      "metadata": {
        "id": "aD-WsKd6g6pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_model_weights(model_path, hyperparams, hyperparams_features, model_type, h5=False):\n",
        "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
        "    dependencies = {\n",
        "    'f1_m': metrics_class.f1_m,\n",
        "    'precision_m': metrics_class.precision_m,\n",
        "    'recall_m': metrics_class.recall_m,\n",
        "    }\n",
        "    loaded_sentEncoder = initialize_model(hyperparams, hyperparams_features, model_type)\n",
        "    loaded_sentEncoder.summary()\n",
        "    path = model_path + \"_weights\"\n",
        "    by_name = False\n",
        "    if h5:\n",
        "        path += \".h5\"\n",
        "        by_name=True\n",
        "    #loaded_model.load_weights(path, by_name=by_name)\n",
        "    loaded_sentEncoder.load_weights(path, by_name=True)\n",
        "    return loaded_sentEncoder"
      ],
      "metadata": {
        "id": "WpxHJyAChNYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing models (takes 3,5 mins)\n",
        "\n",
        "hyperparams['optimizer'] = optimizers.legacy.Adam(learning_rate=hyperparams['lr'],\n",
        "                                                  decay = hyperparams['decay'])\n",
        "hyperparams['batch_size'] = 1\n",
        "\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"\n",
        "\n",
        "han_path = root_dir + '/Final Trained Models (10 epochs)/Depression/Depression_HAN_2023-07-31 23:35:39.671308'\n",
        "bert_path = root_dir +  '/Final Trained Models (10 epochs)/Depression/Depression_HAN_BERT_2023-08-01 18:29:44.243405'\n",
        "conHan_path = root_dir + '/Final Trained Models (10 epochs)/Depression/Depression_Con_HAN_2023-07-28 23:52:39.351657'\n",
        "\n",
        "# model_type = \"HAN\"\n",
        "# han_model, sentEncoder_HAN = load_saved_model_weights(han_path, hyperparams, hyperparams_features, model_type, h5=True)\n",
        "\n",
        "# model_type = \"HAN_BERT\"\n",
        "# sentEncoder_BERT = load_saved_model_weights(bert_path, hyperparams, hyperparams_features, model_type, h5=True)\n",
        "\n",
        "model_type = \"Con_HAN\"\n",
        "sentEncoder_BERT = load_saved_model_weights(conHan_path, hyperparams, hyperparams_features, model_type, h5=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "7V7Gw4V0lZSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "def encode_text_BERT(tokens):\n",
        "  tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny',\n",
        "                                                      do_lower_case=True)\n",
        "  encodings = tokenizer(tokens, add_special_tokens=True, max_length=hyperparams['maxlen'],\n",
        "                              padding='max_length', truncation=True,\n",
        "                              return_attention_mask=True, is_split_into_words=True\n",
        "                        )\n",
        "  encoded_token_ids = encodings['input_ids']\n",
        "  encoded_token_attnmasks = encodings['attention_mask']\n",
        "  encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
        "  encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
        "  encoded_stopwords = encode_stopwords(tokens, stopwords_list)\n",
        "  encoded_liwc = encode_liwc_categories(tokens, liwc_categories, liwc_dict)\n",
        "\n",
        "  user_token_ids = encoded_token_ids\n",
        "  user_token_attnmasks = encoded_token_attnmasks\n",
        "\n",
        "  user_categ_data = [[encoded_emotions + [encoded_pronouns] + encoded_liwc]]\n",
        "  user_sparse_data = [[encoded_stopwords]]\n",
        "\n",
        "  return (user_token_ids, user_token_attnmasks, user_categ_data, user_sparse_data)\n",
        "\n",
        "\n",
        "def encode_text_HAN(tokens):\n",
        "  # Using voc_size-1 value for OOV token\n",
        "  encoded_tokens = [vocabulary.get(w, hyperparams_features['max_features'] - 1) for w in tokens]\n",
        "  encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
        "  encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
        "  encoded_stopwords = encode_stopwords(tokens, stopwords_list)\n",
        "  encoded_liwc = encode_liwc_categories(tokens, liwc_categories, liwc_dict)\n",
        "\n",
        "  user_tokens = [np.array(pad_sequences([encoded_tokens], maxlen=hyperparams['maxlen'],\n",
        "                                                      padding=\"pre\",\n",
        "                                                      truncating=\"pre\"))]\n",
        "  user_categ_data = [[encoded_emotions + [encoded_pronouns] + encoded_liwc]]\n",
        "  user_sparse_data = [[encoded_stopwords]]\n",
        "\n",
        "  return (user_tokens, user_categ_data, user_sparse_data\n",
        "          )\n",
        "\n",
        "def get_post_attentions(sentence, model, model_type):\n",
        "\n",
        "  if model_type == \"HAN\":\n",
        "    sentence = sentence[0]\n",
        "  elif model_type == \"HAN_BERT\":\n",
        "    sentence_ids = np.array(sentence[0]).reshape((1, -1))\n",
        "    sentence_masks = np.array(sentence[1]).reshape((1, -1))\n",
        "    print(sentence_ids)\n",
        "    print(sentence_masks)\n",
        "\n",
        "  attention_layer = model.get_layer(\"attention\")\n",
        "\n",
        "  attention_function = tf.keras.backend.function(inputs=[model.input], outputs=[attention_layer.output])\n",
        "\n",
        "  if model_type == \"HAN\":\n",
        "    predictions = model.predict(sentence, verbose=False)\n",
        "    attention_weights = np.squeeze(attention_function(sentence)[0])\n",
        "  elif model_type == \"HAN_BERT\":\n",
        "    predictions = model.predict([sentence_ids, sentence_masks], verbose=False)\n",
        "    attention_weights = np.squeeze(attention_function([sentence_ids, sentence_masks])[0])\n",
        "\n",
        "  attention_weights = np.exp(attention_weights)\n",
        "  attention_weights = attention_weights / sum(attention_weights)\n",
        "\n",
        "  return attention_weights\n"
      ],
      "metadata": {
        "id": "z2d-uOGMpilt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am feeling very blue, despite wearing a blue t-shirt\".split()\n",
        "hyperparams['batch_size'] = 1\n",
        "\n",
        "\n",
        "# HAN_encoded = encode_text_HAN(sentence)\n",
        "# weights_HAN = get_post_attentions(HAN_encoded, sentEncoder_HAN, \"HAN\")[-len(sentence):]\n",
        "\n",
        "BERT_encoded = encode_text_BERT(sentence)\n",
        "weights_BERT = get_post_attentions(BERT_encoded, sentEncoder_BERT, \"HAN_BERT\")\n",
        "print(weights_BERT)\n",
        "\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "  print(f\"{word}: {weights_BERT[i+1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E4qDppLQt29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing multi-head self-attention"
      ],
      "metadata": {
        "id": "_80pO_zpb4U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading model (DEPRESSION)\n",
        "from load_save_model import load_saved_model_weights\n",
        "\n",
        "hyperparams['optimizer'] = optimizers.legacy.Adam(learning_rate=hyperparams['lr'],\n",
        "                                                  decay = hyperparams['decay'])\n",
        "hyperparams['batch_size'] = 1\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Thesis/Data\"\n",
        "\n",
        "model_type = \"Con_HAN\"\n",
        "ConHan_path = root_dir + '/Final Trained Models (10 epochs)/Depression/Depression_Con_HAN_2023-07-28 23:52:39.351657'\n",
        "\n",
        "ConHan_model = load_saved_model_weights(ConHan_path, hyperparams, hyperparams_features, model_type, h5=True)"
      ],
      "metadata": {
        "id": "HAHn5xnKcB9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_generator import DataGenerator_BERT\n",
        "#Create DataGen\n",
        "data_gen= DataGenerator_BERT(user_level_data, subjects_split, set_type='test',\n",
        "                                          hyperparams_features=hyperparams_features, model_type=model_type,\n",
        "                                          seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
        "                                          posts_per_group=hyperparams['posts_per_group'],\n",
        "                                          post_groups_per_user=None,\n",
        "                                          max_posts_per_user=hyperparams['posts_per_user'],\n",
        "                                          compute_liwc=True,\n",
        "                                          ablate_emotions='emotions' in hyperparams['ignore_layer'],\n",
        "                                          ablate_liwc='liwc' in hyperparams['ignore_layer'])\n"
      ],
      "metadata": {
        "id": "y3wFWDCKiGGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-tiny',\n",
        "                                                           do_lower_case=True)\n",
        "\n",
        "#returns the texts of a chunk in a dict\n",
        "def text_dict(x):\n",
        "  token_ids = x[0][0]\n",
        "  texts = {}\n",
        "\n",
        "  for i in range(token_ids.shape[0]):\n",
        "    ids = token_ids[i,:]\n",
        "    text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "    texts[str(i)] = text\n",
        "\n",
        "  return texts"
      ],
      "metadata": {
        "id": "oMtfhRzWL4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "printed = 0\n",
        "\n",
        "print_top = 3\n",
        "\n",
        "for iter, (x, y) in enumerate(data_gen):\n",
        "  if y == 1: #only check positives\n",
        "    printed+=1\n",
        "    print(iter)\n",
        "\n",
        "    texts = text_dict(x)\n",
        "\n",
        "    #obtaining weights of the final attention layer.\n",
        "    attention_input = tf.keras.backend.function(inputs=[ConHan_model.input], outputs=[ConHan_model.get_layer(\"MH-attention_layer_0\").output])(x)[0]\n",
        "    attention_layer = ConHan_model.get_layer(\"MH-attention_layer_1\")\n",
        "    _, attention_scores = attention_layer(attention_input, attention_input, return_attention_scores=True)\n",
        "\n",
        "    # Reshape attention_scores from (1, 3, 50, 50) to (3, 50, 50)\n",
        "    attention_scores = tf.squeeze(attention_scores, axis=0)\n",
        "\n",
        "    # Create a figure with 3 subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Loop through attention scores and create subplots\n",
        "\n",
        "    saved_texts = {}\n",
        "\n",
        "    for i, ax in enumerate(axs):\n",
        "        ax.set_title(f\"Head {i+1}\")\n",
        "        attention_map = attention_scores[i]\n",
        "\n",
        "        #Printing the texts of the 3 highest attention posts of this head\n",
        "        total_att_per_post = np.sum(attention_map, axis=0)\n",
        "        highest_att_posts = np.argsort(total_att_per_post)[-print_top:]\n",
        "\n",
        "        for index in highest_att_posts:\n",
        "          print(f\"Post {index}: {texts[str(index)]}\")\n",
        "\n",
        "          if str(index) not in saved_texts.keys():\n",
        "            saved_texts[str(index)] = texts[str(index)]\n",
        "\n",
        "        im = ax.imshow(attention_map, cmap='Blues')  # You can use other colormaps as well\n",
        "\n",
        "        # Add numbers to horizontal and vertical axes\n",
        "        ax.set_xticks(np.arange(0, 50, 5))  # Add ticks at positions 0, 10, 20, ..., 40\n",
        "        ax.set_yticks(np.arange(0, 50, 10))  # Add ticks at positions 0, 10, 20, ..., 40\n",
        "        ax.set_xticklabels(np.arange(0, 50, 5))  # Label ticks as 1, 11, 21, ..., 50\n",
        "        ax.set_yticklabels(np.arange(50, 0, -10))  # Label ticks as 50, 40, 30, ..., 1\n",
        "\n",
        "        ax.axis('on')  # Turn axes back on\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar_ax = fig.add_axes([0.95, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
        "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
        "    cbar.set_label('Attention Score', rotation=270, labelpad=15)  # Add a label above the colorbar\n",
        "\n",
        "    # Avoid using plt.tight_layout()\n",
        "    plt.subplots_adjust(right=0.92, wspace=0.1)  # Adjust the right side and the spacing between subplots\n",
        "\n",
        "    #saving texts and plot\n",
        "    path = root_dir  + '/Data Analysis/AttPlots/'\n",
        "\n",
        "    plt.savefig(path+str(iter)+'_attention_plot.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    with open(path+str(iter)+'_attention_texts.json', 'w') as file:\n",
        "      json.dump(saved_texts, file)\n",
        "\n",
        "  if printed==20:\n",
        "    break"
      ],
      "metadata": {
        "id": "3k7ybewGrI9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}